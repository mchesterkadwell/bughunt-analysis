{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing the Bughunt Corpus\n",
    "This notebook follows the process of taking the manually cleaned Bughunt corpus and creating a frequency distribution of the different insect words. The frequency distribution is output in csv format, which is used in the next notebook to create the visualisation.\n",
    "\n",
    "We will use the code library called Natural Language Toolkit (NLTK) to provide a lot of text mining functions that are already written. More information on this can be found here: http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Corpus Files\n",
    "\n",
    "We already have the corpus **split into files by decade**. Here is a list of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1800.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1810.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1820.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1830.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1840.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1850.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1860.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1870.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1880.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1890.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1900.txt'),\n",
       " PosixPath('../corpora/bughunt/2-clean-by-decade/bughunt-clean-1910.txt')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_path = Path('..', 'corpora', 'bughunt', '2-clean-by-decade')\n",
    "files = [Path(root, filename) for root, _, files in os.walk(data_path) for filename in files]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Process\n",
    "Before we are ready to process these files, we need to gather together some resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug Words\n",
    "We have our list of **simple bug words** as a text file. Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ant',\n",
       " 'bee',\n",
       " 'beetle',\n",
       " 'butterfly',\n",
       " 'cockroach',\n",
       " 'cricket',\n",
       " 'dragonfly',\n",
       " 'earwig',\n",
       " 'flea',\n",
       " 'fly',\n",
       " 'gnat',\n",
       " 'grasshopper',\n",
       " 'ladybird',\n",
       " 'louse',\n",
       " 'mosquito',\n",
       " 'moth',\n",
       " 'spider',\n",
       " 'termite',\n",
       " 'wasp']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = Path('..', 'wordlists', 'insect-wordlist.txt')\n",
    "with open(wordlist) as reader:\n",
    "    bug_words = reader.read().splitlines()\n",
    "bug_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a list of the **stems** of bug words. Stemming means reducing a word to its root, eliminating plurals and other inflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ant',\n",
       " 'bee',\n",
       " 'beetl',\n",
       " 'butterfli',\n",
       " 'cockroach',\n",
       " 'cricket',\n",
       " 'dragonfli',\n",
       " 'earwig',\n",
       " 'flea',\n",
       " 'fli',\n",
       " 'gnat',\n",
       " 'grasshopp',\n",
       " 'ladybird',\n",
       " 'lous',\n",
       " 'mosquito',\n",
       " 'moth',\n",
       " 'spider',\n",
       " 'termit',\n",
       " 'wasp']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemlist = Path('..', 'wordlists', 'insect-wordstems.txt')\n",
    "with open(stemlist) as reader:\n",
    "    bug_stems = reader.read().splitlines()\n",
    "bug_stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Stopwords\n",
    "We are not interested in common words in English that carry little meaning, such as 'the', 'a' and 'its'. There is no definitive list of stopwords, but a commonly-used list is provided by the Natural Language Toolkit (NLTK)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ../nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', download_dir=Path('..', 'nltk_data'))\n",
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "english_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Puctuation\n",
    "Punctuation such as commas, fullstops and apostrophes can complicate processing a corpus. For example, if punctuation is left in, when we come to split the text into tokens, the words \"termite\" and \"termite,\" might be considered to be different words.\n",
    "\n",
    "This is a complicated matter, however, and what you choose to do would vary depending on the nature of your corpus and what questions you wish to ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
